{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Hello World</p>"},{"location":"computerscience/apimanagement/reference/","title":"API Connect References","text":"<p>Cloud Pak Production Deployment Guides</p>","tags":["reference"]},{"location":"computerscience/apimanagement/apiconnect/","title":"API Management","text":"<p>Hello API Management</p>"},{"location":"computerscience/apimanagement/apiconnect/architecture/active-active-dr/","title":"API Connect Active-Active Disaster Recovery Architecture","text":"","tags":["apiconnect"]},{"location":"computerscience/apimanagement/apiconnect/architecture/active-active-dr/#what-is-disaster-recovery","title":"What is Disaster Recovery?","text":"<p>Disaster recovery is the process of maintaining or reestablishing vital infrastructure and systems following</p> <p>a natural or human-induced disaster, such as a storm or battle. It employs policies, tools, and procedures. (Wikipedia)</p> <p>As you can see the description of DR(Disaster Recovery) from Wikipedia, that is extremely an important step in case of emergency. However, most of the </p> <p>companies push DR into the background. API Connect or similar gateway solutions take an important place sits between client and backend services. </p> <p>Therefore, considering DR solutions also for API Connect is critical in order to prevent unexpected outages.</p>","tags":["apiconnect"]},{"location":"computerscience/apimanagement/apiconnect/architecture/active-active-dr/#official-approach-of-api-connect-for-disaster-recovery","title":"Official Approach of API Connect for Disaster Recovery","text":"<p>Installing a two data center deployment model is the official and best approach for DR solution as documented: https://github.com/ibm-apiconnect/</p> <p>apic-hybrid-cloud-enablement/blob/master/docs-and-tools/architecture/APIC-HA-Architectures.md </p> <p>However,  best practices might not be the  best solution in some of the cases because of the complexity, lack of computing resources or in some other </p> <p>cases, clients or customers just would not like to achieve what is proposed. So that additional approaches or solutions are required for problems.</p>","tags":["apiconnect"]},{"location":"computerscience/apimanagement/apiconnect/architecture/active-active-dr/#what-is-the-problem","title":"What is the problem?","text":"","tags":["apiconnect"]},{"location":"computerscience/apimanagement/apiconnect/architecture/active-active-dr/#scenario-1-prod-cluster-is-the-active-100","title":"Scenario 1 - Prod Cluster is the Active (%100)","text":"","tags":["apiconnect"]},{"location":"computerscience/apimanagement/apiconnect/architecture/active-active-dr/#scenario-2-prod-cluster80-and-dr-cluster20","title":"Scenario 2 - Prod Cluster(%80) and DR Cluster(%20)","text":"","tags":["apiconnect"]},{"location":"computerscience/apimanagement/apiconnect/architecture/active-active-dr/#scenario-3-prod-cluster-is-down-dr-cluster100","title":"Scenario 3 - Prod Cluster is down, DR Cluster(%100)","text":"","tags":["apiconnect"]},{"location":"computerscience/apimanagement/apiconnect/architecture/active-active-dr/#scenario-4-prod-cluster-is-down-dr-cluster100-is-restored","title":"Scenario 4 - Prod Cluster is down, DR Cluster(%100) is restored","text":"","tags":["apiconnect"]},{"location":"computerscience/apimanagement/apiconnect/management/authenticationandsecurity/tls/","title":"TLS Profiles","text":"<p>API Connect supports TLS Profiles for securing data transmission over HTTPS.</p> <p></p>","tags":["tag1","tag2"]},{"location":"computerscience/apimanagement/apiconnect/management/authenticationandsecurity/tls/#type-of-tls-profiles","title":"Type of TLS Profiles","text":"<ol> <li> <p>TLS Server Profile : Gateway uses to configure its endpoint during API execution.</p> </li> <li> <p>TLS Client Profile : System uses to connect with another endpoint over TLS.</p> </li> </ol>","tags":["tag1","tag2"]},{"location":"computerscience/apimanagement/apiconnect/management/authenticationandsecurity/tls/#components-of-tls-profiles","title":"Components of TLS Profiles","text":"<ol> <li> <p>TLS protocol version (1.0,1.1,1.2,1.3)</p> </li> <li> <p>Cipher suites to secure HTTPS communication within the API Connect system.</p> </li> <li> <p>Keystores contain public and private key pairs.</p> </li> <li> <p>Truststores contain public keys for trusted 3rd party services.</p> </li> </ol>","tags":["tag1","tag2"]},{"location":"computerscience/apimanagement/apiconnect/management/authenticationandsecurity/tls/#viewing-tls-profiles","title":"Viewing TLS Profiles?","text":"","tags":["tag1","tag2"]},{"location":"computerscience/apimanagement/apiconnect/management/authenticationandsecurity/tls/#procedure","title":"Procedure:","text":"<ol> <li> <p>Login to the Cloud Manager UI</p> </li> <li> <p>Click Resources</p> </li> <li> <p>Select TLS</p> </li> </ol>","tags":["tag1","tag2"]},{"location":"computerscience/apimanagement/apiconnect/management/authenticationandsecurity/tls/#creating-tls-server-profile","title":"Creating TLS Server Profile","text":"","tags":["tag1","tag2"]},{"location":"computerscience/apimanagement/apiconnect/management/authenticationandsecurity/tls/#procedure_1","title":"Procedure:","text":"<ol> <li> <p>Login to the Cloud Manager UI</p> </li> <li> <p>Click Resources</p> </li> <li> <p>Select TLS</p> </li> <li> <p>Click Create </p> </li> <li> <p>Enter the fields to configure. (WIP-Add SC)</p> </li> </ol>","tags":["tag1","tag2"]},{"location":"computerscience/apimanagement/apiconnect/management/authenticationandsecurity/tls/#creating-tls-client-profile","title":"Creating TLS Client Profile","text":"<ol> <li> <p>Login to the Cloud Manager UI</p> </li> <li> <p>Click Resources</p> </li> <li> <p>Select TLS</p> </li> <li> <p>Click Create </p> </li> <li> <p>Enter the fields to configure. (WIP-Add SC)</p> </li> </ol>","tags":["tag1","tag2"]},{"location":"computerscience/apimanagement/apiconnect/management/backupandrestore/backup/","title":"API Connect Backup Configuration","text":"<p>Backup and restore for API Connect subsystems are daily tasks that you have to manage to keep the systems healthy. </p> <p>API Connect v10 uses Kubernetes operators to backup and restore  databases of the subsystems. </p> <p>Type of the backups are manuel and scheduled.</p>","tags":["apiconnect"]},{"location":"computerscience/apimanagement/apiconnect/management/backupandrestore/backup/#manuel-backups","title":"Manuel backups","text":"<p>For the subsystems, custom resources(CR) are used for the backup configuration. </p> <p>Therefore, you create a backup custom resource(CR) manually.</p> <p>An example for management subsystem in OpenShift:</p> <ol> <li>Check the subsystem is healthy in OpenShift:</li> </ol> <pre><code>oc get &lt;subsystem-name&gt; </code></pre> <ol> <li>Create a backup custom resource(CR) in a backup.yaml file:</li> </ol> <p><pre><code>apiVersion: management.apiconnect.ibm.com/v1beta1\nkind: ManagementBackup\nmetadata:\ngenerateName:&lt;mnagement-subsystem-name&gt;\nspec:\ntype: full\ncrType: create\nclusterName: management\n</code></pre> 3.  Create custom resource in OpenShift in a yaml file:</p> <pre><code>oc create -f backup.yaml -n &lt;management-namaspace&gt;\n</code></pre> <ol> <li>List backups by using OpenShift cli:</li> </ol> <pre><code>oc get managementbackup -n &lt;management-namespace&gt;\n</code></pre>","tags":["apiconnect"]},{"location":"computerscience/apimanagement/apiconnect/management/backupandrestore/backup/#scheduled-backups","title":"Scheduled backups","text":"<p>In order to trigger backups whenever you would like to, you create a cron schedule and configure backup configurations</p> <p>in the management custom resource(CR).</p> <p>An example for management subsystem in OpenShift:</p> <ol> <li>Create backup secret in OpenShift to provide credentials that is used in sftp connection:</li> </ol> <pre><code>oc create secret generic &lt;management-backup-secret-name&gt; --from-literal=username='&lt;username&gt;' --from-literal=password='&lt;password&gt;' -n &lt;management-namespace&gt;\n</code></pre> <ol> <li>Edit management subsystem custom resource(CR).</li> </ol> <pre><code>oc edit &lt;management-custom-resource-name&gt;\n</code></pre> <ol> <li>Add scheduled backup configuration to management custom resource yaml file:</li> </ol> <pre><code>spec:\n  management:\n    databaseBackup:\n      protocol: sftp\n      host: &lt;sftp-host-name&gt;\n      port: &lt;sftp-port&gt;\n      path: &lt;backup-location&gt;\n      retries: 0\ncredentials: &lt;management-backup-secret-name&gt;\n      schedule: \"30 11 * * *\"\n</code></pre> <p>4) Check the backup location if the scheduled backup configuration works as expected:</p>","tags":["apiconnect"]},{"location":"computerscience/apimanagement/apiconnect/management/backupandrestore/backup/#troubleshooting-the-backup-problems","title":"Troubleshooting the backup problems","text":"<ol> <li>Find the pod name that includes string backrest-shared-repo in OpenShift. </li> </ol> <p>If you are not sure about the namespace, you can use -A option to search for all namespaces.</p> <pre><code>oc get po -n &lt;management-namespace&gt; | grep backrest-shared-repo\n</code></pre> <ol> <li>Open a remote shell to pod:</li> </ol> <pre><code>oc rsh &lt;backrest-shared-repo-pod-name&gt;\n</code></pre> <ol> <li>Create a tmp file in /tmp:</li> </ol> <pre><code>touch tmp_file\n</code></pre> <ol> <li>SFTP to sftp server:</li> </ol> <pre><code>sftp -P 22 &lt;username&gt;@&lt;sftp-server-name&gt;\n</code></pre> <ol> <li>Change directory to the backup location that we configured in custom resource file:</li> </ol> <pre><code>cd &lt;backup-location&gt;\n</code></pre> <ol> <li>Transfer tmp file to the sftp server backup location:</li> </ol> <p><pre><code>put tmp_file\n</code></pre> 7. Check if the file is transferred successfully:</p> <pre><code>ls -l\n</code></pre>","tags":["apiconnect"]},{"location":"computerscience/apimanagement/apiconnect/management/backupandrestore/backup/#steps-to-check-if-you-face-a-problem-during-sftp-connection","title":"Steps to check if you face a problem during sftp connection:","text":"<ol> <li> <p>Check the user has write access to the backup location that you configured.</p> </li> <li> <p>Check if the sftp daemon is running or not on the sftp server.</p> </li> </ol>","tags":["apiconnect"]},{"location":"computerscience/apimanagement/apiconnect/management/formfactormigration/formfactormigration/","title":"Migrating from v10 to v10 on a different form factor","text":"","tags":["apiconnect"]},{"location":"computerscience/apimanagement/apiconnect/management/formfactormigration/formfactormigration/#reference","title":"Reference","text":"<p>https://www.ibm.com/docs/en/api-connect/10.0.5.x_lts?topic=connect-migrating-from-v10-v10-different-form-factor</p>","tags":["apiconnect"]},{"location":"computerscience/apimanagement/apiconnect/management/formfactormigration/formfactormigration/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Install OCP.</p> </li> <li> <p>Create secret with entitlement key.</p> </li> <li> <p>Install  pyyaml</p> </li> </ol> <p><pre><code>pip3 install pyyaml\n</code></pre> 4. Install apic cli.</p> <ol> <li>Ensure no connectivity between Production and DR sites.</li> </ol>","tags":["apiconnect"]},{"location":"computerscience/apimanagement/apiconnect/management/formfactormigration/formfactormigration/#form-factor-migration-steps","title":"Form Factor Migration Steps:","text":"<ol> <li>Prepare source system: </li> </ol> <pre><code>python3 save_v10_source_configuration.py -u admin -p  -s cpd-cp4i.apps.cp4int.example.com/integration/apis/apic/apic -r admin/default-idp-1 -n apic </code></pre> <ol> <li>Create secrets in target:</li> </ol> <pre><code>python3 create_secrets_in_target.py -n apic\n</code></pre> <ol> <li>Prepare top level CR:</li> </ol> <pre><code>oc get apiconnectcluster apic --show-managed-fields=false -o yaml\n</code></pre> <p>Top Level CR</p> <pre><code>apiVersion: apiconnect.ibm.com/v1beta1\nkind: APIConnectCluster\nmetadata:\nlabels:\napp.kubernetes.io/instance: apiconnect\napp.kubernetes.io/managed-by: ibm-apiconnect\napp.kubernetes.io/name: apiconnect-production\nname: apic\nnamespace: apic\nspec:\nanalytics:\nstorage:\nenabled: true\ntype: unique\nlicense:\naccept: true\nlicense: L-RJON-CEBLEH\nmetric: VIRTUAL_PROCESSOR_CORE\nuse: production\nimagePullSecrets:\n- ibm-entitlement-key\ngateway:\nadminUser: {}\napicGatewayPeeringTLS: {}\napicGatewayServiceTLS: {}\ngatewayEndpoint:\nhosts:\n- name: apidr.example.com\nsecretName: apigwcustom-cert\ngatewayManagerEndpoint: {}\nimageRegistry: cp.icr.io/cp/apic\nmanagement:\ncustomApplicationCredentials:\n- name: atm-cred\nsecretName: apic-mgmt-atm-cred\n- name: ccli-cred\nsecretName: apic-mgmt-ccli-cred\n- name: cui-cred\nsecretName: apic-mgmt-cui-cred\n- name: dsgr-cred\nsecretName: apic-mgmt-dsgr-cred\n- name: juhu-cred\nsecretName: apic-mgmt-juhu-cred\n- name: cli-cred\nsecretName: apic-mgmt-cli-cred\n- name: ui-cred\nsecretName: apic-mgmt-ui-cred\ndatabaseBackup:\ncredentials: mgmt-backup-secret\nhost: sftp.example.com\npath: /\nport: 22\nprotocol: sftp\nrestartDB:\naccept: false\nencryptionSecret:\nsecretName: apic-mgmt-enc-key\nname: apic-mgmt\nportal:\nadminClientSubjectDN: ''\nportalAdminEndpoint: {}\nportalUIEndpoint:\nhosts:\n- name: devportaldr.example.com\nsecretName: portalcustom-cert\nencryptionSecret:\nsecretName: apic-ptl-enc-key\nportalBackup:\ncredentials: mgmt-backup-secret\nhost: sftp.example.com\npath: /\nport: 22\nprotocol: sftp\nprofile: n3xc14.m48\nstorageClassName: ocs-storagecluster-ceph-rbd\nversion: 10.0.5.0\n</code></pre> <ol> <li> <p>Create apigwcustom-cert certificate in OCP.</p> </li> <li> <p>Create portalcustom-cert certificate in OCP.</p> </li> <li> <p>Install target system with python script:</p> </li> </ol> <pre><code>python3 install_apic_on_ocp.py \n-license L-RJON-CEBLEH\n-n apic \n-name apic \n-storageclass_apic ocs-storagecluster-ceph-rbd \n-cp4i\n-storageclass_pn ocs-storagecluster-ceph-fs \n-license_pn L-RJON-CD3JKX \n-production \n-profile n3xc14.m48\n</code></pre> <ol> <li> <p>Wait for operands to be installed.</p> </li> <li> <p>Login to Cloud Manager and crete organization.</p> </li> <li> <p>Ensure that apic is installed on path</p> </li> <li> <p>Accept the licence and check apic login before restoring the target system:</p> </li> </ol> <pre><code>apic login --realm admin/default-idp-1 --username admin --server apic-mgmt-platform-api-apic.apps.cp4intdr.example.com --password </code></pre> <ol> <li>Restore target system:</li> </ol> <pre><code>python3 restore_management_db.py -n apic -s apic-mgmt-platform-api-apic.apps.cp4intdr.example.com\n</code></pre> <p>Note: Continue to the steps, don't login before running register_gateway_portal script.</p> <ol> <li>Change route settings in OCP:</li> </ol> <p>apic-gw-gateway-apic.apps.cp4intdr.example.com -&gt; apidr.example.com</p> <p>apic-gw-gateway-apic.apps.cp4intdr.example.com -&gt; devportal.example.com</p> <ol> <li>Prepare gateway_portal_mappings.yaml:</li> </ol> <pre><code>analytics_mapping:\nanalytics-service:\nhttps://apic-a7s-ai-endpoint-apic.apps.cp4int.example.com: https://apic-a7s-ai-endpoint-apic.apps.cp4intdr.example.com\ngateway_mapping:\napi-gateway-service:\nhttps://api.example.com/: https://apidr.example.com/\nhttps://apic-gw-gateway-manager-apic.apps.cp4int.example.com: https://apic-gw-gateway-manager-apic.apps.cp4intdr.example.com\nportal_mapping:\nportal-service:\nhttps://apic-ptl-portal-director-apic.apps.cp4int.example.com: https://apic-ptl-portal-director-apic.apps.cp4intdr.example.com\n</code></pre> <ol> <li>Prepare provider organizations credentials.yaml</li> </ol> <pre><code>provider_org_credentials:\napiManagerHostName: https://apic-mgmt-platform-api-apic.apps.cp4intdr.example.com\napi:\npassword: realm: provider/default-idp-2\nusername: admin\nuseSameCredentialsForAllProviderOrgs: true\n</code></pre> <ol> <li> <p>Check connectivity between Portal Admin Endpoint and UI Endpoint and APIM pods.</p> </li> <li> <p>Run register_gateway_portal script:</p> </li> </ol> <p>```bash python3 register_gateway_portals_in_target.py -n apic -u admin -p -s apic-mgmt-platform-api-apic.apps.cp4intdr.example.com -r admin/default-idp-1 -silent <pre><code>17. Login to Cloud Manager and check new portal and gw services or check them with apic cli:\n\n```bash\napic login --server apic-mgmt-platform-api-apic.apps.cp4intdr.example.com --realm admin/default-idp-1 --username admin -p\n</code></pre></p> <pre><code>apic orgs:list --org_type=admin --fields id,name --server apic-mgmt-platform-api-apic.apps.cp4intdr.example.com\n</code></pre> <pre><code>apic portal-services:list --fields name,endpoint --server apic-mgmt-platform-api-apic.apps.cp4intdr.example.com --org admin --availability-zone availability-zone-default\n</code></pre> <pre><code>apic gateway-services:list --fields name,endpoint --server apic-mgmt-platform-api-apic.apps.cp4intdr.example.com --org admin --availability-zone availability-zone-default\n</code></pre> <ol> <li>Install dnsmasq to the server which is accessible from DR cluster.</li> </ol> <pre><code>yum install dnsmasq\n</code></pre> <p>Important: New version of python scripts have --force to overcome this. DNSMasq might not be needed.</p> <ol> <li>Add below lines to dnsmasq</li> </ol> <pre><code>address=/cpd-cp4i.apps.cp4int.example.com/10.1.36.121\naddress=/cp-console.apps.cp4int.example.com/10.1.36.121\naddress=/cp-proxy.apps.cp4int.example.com/10.1.36.121\naddress=/apic-a7s-ai-endpoint-apic.apps.cp4int.example.com/10.1.36.121\naddress=/apic-mgmt-api-manager-apic.apps.cp4int.example.com/10.1.36.121\naddress=/apic-mgmt-admin-apic.apps.cp4int.example.com/10.1.36.121\naddress=/apic-mgmt-consumer-api-apic.apps.cp4int.example.com/10.1.36.121\naddress=/apic-mgmt-platform-api-apic.apps.cp4int.example.com/10.1.36.121\naddress=/apic-ptl-portal-director-apic.apps.cp4int.example.com/10.1.36.121\naddress=/devportal.example.com/10.1.36.121\naddress=/api.example.com/10.1.36.121\naddress=/apic-gw-gateway-manager-apic.apps.cp4int.example.com/10.1.36.121\n</code></pre> <ol> <li>Test dnsmasq configuration from the server which has dnsmasq.</li> </ol> <p>```bash dig @127.0.0.1 cpd-cp4i.apps.cp4int.example.com <pre><code>21. Get eth ip to set listen_address from the dnsmasq server.\n\n```bash\nifconfig\n</code></pre></p> <p><pre><code>listen-address=10.1.36.21\n</code></pre> 22. Test dnsmasq configuration. </p> <p>dig @10.1.36.21 cpd-cp4i.apps.cp4int.example.com</p> <ol> <li>List DNS operator in OCP:</li> </ol> <pre><code>oc get DNS.operator default -o yaml\n</code></pre> <ol> <li>Add below lines to dns-config.yaml</li> </ol> <pre><code>apiVersion: operator.openshift.io/v1\nkind: DNS\nmetadata:\nname: default\nspec:\nservers:\n- name: temp-dns\nzones: - cpd-cp4i.apps.cp4int.example.com\n- cp-console.apps.cp4int.example.com\n- cp-proxy.apps.cp4int.example.com\n- apic-a7s-ai-endpoint-apic.apps.cp4int.example.com\n- apic-mgmt-api-manager-apic.apps.cp4int.example.com\n- apic-mgmt-admin-apic.apps.cp4int.example.com\n- apic-mgmt-admin-apic.apps.cp4int.example.com\n- apic-mgmt-consumer-api-apic.apps.cp4int.example.com\n- apic-mgmt-platform-api-apic.apps.cp4int.example.com\n- apic-ptl-portal-director-apic.apps.cp4int.example.com\n- devportal.example.com\n- api.example.com  - apic-gw-gateway-manager-apic.apps.cp4int.example.com\nforwardPlugin:\npolicy: Random upstreams: - 10.1.36.21\n```\n\n25. Apply YAML file.\n\n```bash\noc apply -f dns-config.yaml\n</code></pre> <ol> <li> <p>Restart dns-operator pod.</p> </li> <li> <p>Run update_to_new_portals.py</p> </li> </ol> <pre><code>  python3 update_to_new_portals.py -n apic -u admin -p -s apic-mgmt-platform-api-apic.apps.cp4intdr.example.com -r admin/default-idp-1 -silent -api_manager_hostname  apic-mgmt-platform-api-apic.apps.cp4intdr.example.com\n</code></pre> <p>Duration: 20-30 minutes</p> <ol> <li>Run update_to_new_gateways.py</li> </ol> <pre><code>  python3 update_to_new_gateways.py -n apic -u admin -p -s apic-mgmt-platform-api-apic.apps.cp4intdr.example.com -r admin/default-idp-1 -silent -api_manager_hostname  apic-mgmt-platform-api-apic.apps.cp4intdr.example.com\n</code></pre>","tags":["apiconnect"]},{"location":"computerscience/apimanagement/apiconnect/management/formfactormigration/formfactormigration/#troubleshooting","title":"Troubleshooting","text":"<ol> <li> <p>OCP Error: https://www.ibm.com/support/pages/cluster-unstable-and-zen-metastoredb-restarting-repeatedly</p> </li> <li> <p>NTP server check:</p> </li> </ol> <p><pre><code>curl -v telnet://ntp.example.com:123  </code></pre> Check chrony services as ODF is not running without NTP</p> <ol> <li> <p>Running some of the python scripts might require --force. (catalog-settings-update)</p> </li> <li> <p>F5 OpenShift ingress router pods </p> </li> </ol>","tags":["apiconnect"]},{"location":"computerscience/apimanagement/apiconnect/management/troubleshooting/troubleshooting/","title":"API Connect Troubleshooting","text":"","tags":["troubleshooting","apiconnect"]},{"location":"computerscience/apimanagement/apiconnect/management/troubleshooting/troubleshooting/#apic-is-not-up-and-components-should-be-up-in-order","title":"APIC is not up and components should be up in order!","text":"<ol> <li> <p>backrest and postgres</p> </li> <li> <p>pgbouncer</p> </li> <li> <p>lur</p> </li> <li> <p>juhu</p> </li> <li> <p>ldap</p> </li> <li> <p>nats</p> </li> <li> <p>stan</p> </li> <li> <p>taskmaager</p> </li> <li> <p>mgmt-ui</p> </li> <li> <p>portal proxy analyt proxy client dowload server</p> </li> <li> <p>websocket proxy </p> </li> <li> <p>apiconnect</p> </li> <li> <p>turnstile</p> </li> <li> <p>hub</p> </li> </ol>","tags":["troubleshooting","apiconnect"]},{"location":"computerscience/apimanagement/apiconnect/management/troubleshooting/troubleshooting/#api-protection-error-while-publishing-apis-on-10054","title":"API Protection error while publishing APIs on 10.0.5.4","text":"<ol> <li> <p>Check api_protection is enabled for webhook: apicops debug:info for all domains.</p> </li> <li> <p>Reregister gateways and apply the steps below:</p> </li> </ol> <p>https://www.ibm.com/docs/en/api-connect/10.0.5.x_lts?topic=acgs-dynamically-re-registering-reconfiguring-gateway-service-in-kubernetes-deployment</p> <ol> <li> <p>Edit GW Services summary to send an update and test management-gateway communication.</p> </li> <li> <p>Check tasks in API Manager Catalog Settings. There should not be any queued tasks.</p> </li> </ol>","tags":["troubleshooting","apiconnect"]},{"location":"computerscience/cloud-computing/openshift/cluster-configuration/audit-logs/","title":"OpenShift Audit Logs","text":"<p>Audit logging records event, event time, responsible user and the impacted entity etc. of your software system or service.</p> <p>OpenShift Audit Logs capture following information for the events:</p> <ul> <li> <p>auditId</p> </li> <li> <p>requestURI</p> </li> <li> <p>verb</p> </li> <li> <p>user</p> </li> <li> <p>sourceIps</p> </li> <li> <p>objectRef</p> </li> <li> <p>responseStatus</p> </li> <li> <p>timestamps</p> </li> </ul> <p>If you have already set audit logging in OpenShift, you may start filtering them to get more meaningful info about the events.</p>","tags":["openshift"]},{"location":"computerscience/cloud-computing/openshift/cluster-configuration/audit-logs/#examples-and-some-common-use-cases-by-filtering-jq","title":"Examples and some common use cases by filtering jq:","text":"<ol> <li>List control plane node's audit logs:</li> </ol> <p><pre><code>oc adm node-logs --role=master --path=openshift-apiserver\n</code></pre> * Change the path according to your requirements.</p> <ol> <li>Print out specific audit log file: <pre><code>oc adm node-logs &lt;node-name&gt; --path=openshift-apiserver/audit.log\n````\n\n3. List Kubernetes API Server audit logs:\n```bash\noc adm node-logs --role=master --path=kube-apiserver\n</code></pre></li> <li> <p>Print out specific node's Kubernetes API Server audit log: <pre><code>oc adm node-logs &lt;node-name&gt; --path=kube-apiserver/audit.log\n````\n\n5. List OpenShift OAuth API Server audit logs:\n```bash\noc adm node-logs --role=master --path=oauth-apiserver\n</code></pre></p> </li> <li> <p>Print out specific node's OpenShift OAuth API Server audit log: <pre><code>oc adm node-logs &lt;node-name&gt; --path=oauth-apiserver/audit.log\n````\n\n7. List OpenShift OAuth Server audit logs:\n```bash\noc adm node-logs --role=master --path=oauth-server\n</code></pre></p> </li> <li> <p>Print out specific node's OpenShift OAuth Server audit log: <pre><code>oc adm node-logs &lt;node-name&gt; --path=oauth-server/audit.log\n</code></pre></p> </li> <li> <p>Filter OpenShift API Server audit logs related to delete requests: <pre><code>oc adm node-logs &lt;node-name&gt; --path=openshift-apiserver/audit.log | jq 'select(.verb == \"delete\")'\n````\n\n10. Filter OpenShift API Server audit logs related to routes:\n```bash\noc adm node-logs &lt;node-name&gt; --path=openshift-apiserver/audit.log | jq 'select(.verb == \"get\" and .objectRef.resource==\"routes\")'\n````\n\n11. Filter all successfull login attempts related to user:\n```bash\noc adm node-logs &lt;node-name&gt; --path=oauth-server/audit.log | jq 'select(.verb==\"get\" and .annotations[\"authentication.openshift.io/decision\"]==\"allow\" and .annotations[\"authentication.openshift.io/username\"]==\"&lt;username&gt;\")'\n````\n\n12. Filter all user creation audit logs:\n```bash\noc adm node-logs &lt;node-name&gt; --path=openshift-apiserver/audit.log | jq 'select(.verb == \"create\" and .objectRef.resource==\"users\")'\n````\n\n13. Filter all service account creation and deletion audit logs:\n```bash\noc adm node-logs &lt;node-name&gt; --path=openshift-apiserver/audit.log | jq 'select((.verb == \"delete\" or .verb == \"create\") and .objectRef.resource==\"serviceaccounts\")'\n````\n\n14. Filter all pod creation and deletion audit logs:\n```bash\noc adm node-logs &lt;node-name&gt; --path=kube-apiserver/audit.log  | jq -r 'select((.verb == \"delete\" or .verb == \"create\") and .objectRef.resource==\"pods\")'\n</code></pre></p> </li> <li> <p>Filter all failed login attempts: <pre><code>oc adm node-logs &lt;node-name&gt; --path=oauth-server/audit.log | jq 'select(.verb==\"get\" and .annotations[\"authentication.openshift.io/decision\"]==\"deny\")'\n````\n\n## Example Audit Log Event:\n```json\n{\n\"kind\": \"Event\",\n  \"apiVersion\": \"audit.k8s.io/v1\",\n  \"level\": \"Metadata\",\n  \"auditID\": \"27a46379-4855-4315-adaf-94493b6888b3\",\n  \"stage\": \"ResponseComplete\",\n  \"requestURI\": \"/apis/route.openshift.io/v1/namespaces/openshift-monitoring/routes/alertmanager-main\",\n  \"verb\": \"get\",\n  \"user\": {\n\"username\": \"system:serviceaccount:openshift-monitoring:cluster-monitoring-operator\",\n    \"groups\": [\n\"system:serviceaccounts\",\n      \"system:serviceaccounts:openshift-monitoring\",\n      \"system:authenticated\"\n],\n    \"extra\": {\n\"authentication.kubernetes.io/pod-name\": [\n\"cluster-monitoring-operator-6dd9848789-4j4f9\"\n],\n      \"authentication.kubernetes.io/pod-uid\": [\n\"7fa68c60-0d32-43dc-a4ba-25f82d21bb34\"\n]\n}\n},\n  \"sourceIPs\": [\n\"x.x.x.x\",\n    \"x.x.x.x\"\n],\n  \"userAgent\": \"operator/v0.0.0 (linux/amd64) kubernetes/$Format\",\n  \"objectRef\": {\n\"resource\": \"routes\",\n    \"namespace\": \"openshift-monitoring\",\n    \"name\": \"alertmanager-main\",\n    \"apiGroup\": \"route.openshift.io\",\n    \"apiVersion\": \"v1\"\n},\n  \"responseStatus\": {\n\"metadata\": {},\n    \"code\": 200\n},\n  \"requestReceivedTimestamp\": \"2023-09-26T19:05:26.595182Z\",\n  \"stageTimestamp\": \"2023-09-26T19:05:26.600763Z\",\n  \"annotations\": {\n\"authorization.k8s.io/decision\": \"allow\",\n    \"authorization.k8s.io/reason\": \"RBAC: allowed by RoleBinding \\\"cluster-monitoring-operator/openshift-monitoring\\\" of ClusterRole \\\"cluster-monitoring-operator-namespaced\\\" to ServiceAccount \\\"cluster-monitoring-operator/openshift-monitoring\\\"\"\n}\n}\n</code></pre></p> </li> </ol> <p>API Index: https://docs.openshift.com/container-platform/4.12/rest_api/index.html</p>","tags":["openshift"]},{"location":"computerscience/cloud-computing/openshift/troubleshooting/troubleshooting/","title":"Update finalizers to null","text":"<p>In order to get rid of the resources while the project is being terminating, update finalizers to null.</p>","tags":["openshift"]},{"location":"computerscience/devops/openshift/references/","title":"OpenShift References","text":"<p>RedHat Customer Portal</p> <p>RedHat OpenShift Documentation</p> <p>RedHat OpenShift Solution Design Guidance</p> <p>OpenShift Examples</p> <p>OpenShift Network Calculator</p>","tags":["reference"]},{"location":"computerscience/devops/openshift/troubleshooting/","title":"OpenShift Troubleshooting","text":"","tags":["troubleshooting","openshift"]},{"location":"computerscience/devops/openshift/troubleshooting/#the-openshift-installation-failed-with-errors","title":"The openshift installation failed with errors","text":"<pre><code>E1011 08:07:56.183305       1 auth.go:231] error contacting auth provider time=\"2023-11-07T17:50:00+03:00\" level=error msg=\"Cluster operator authentication Degraded is True with Ingr wwssStateEndpoints_MissingSubsets::OAuthClientsController_SyncError::OAuthServerDeployment_PreconditionNotFulfilled::OAuthServerRouteEndpointAccessibleController_SyncError::OAuthServerServiceEndpointAccessibleController_SyncError::OAuthServerServiceEndpointsEndpointAccessibleController_SyncError::ProxyConfigController_SyncError::WellKnownReadyController_SyncError: IngressStateEndpointsDegraded: No subsets found for the endpoints of oauth-server\\nOAuthClientsControllerDegraded: no ingress for host oauth-openshift.apps.ocpactest.example.com in route oauth-openshift in namespace openshift-authentication\\nOAuthServerDeploymentDegraded: waiting for the oauth-openshift route to contain an admitted ingress: no admitted ingress for route oauth-openshift in namespace openshift-authentication\\nOAuthServerDeploymentDegraded: \\nOAuthServerRouteEndpointAccessibleControllerDegraded: route \\\"openshift-authentication/oauth-openshift\\\": status does not have a valid host address\\nOAuthServerServiceEndpointAccessibleControllerDegraded: Get \\\"https://10.132.220.152:443/healthz\\\": dial tcp 10.132.220.152:443: connect: connection refused\\nOAuthServerServiceEndpointsEndpointAccessibleControllerDegraded: oauth service endpoints are not ready\\nProxyConfigControllerDegraded: no admitted ingress for route oauth-openshift in namespace openshift-authentication\\nWellKnownReadyControllerDegraded: failed to get oauth metadata from openshift-config-managed/oauth-openshift ConfigMap: configmap \\\"oauth-openshift\\\" not found (check authentication operator, it is supposed to create this)\"\ntime=\"2023-11-07T17:50:00+03:00\" level=error msg=\"Cluster operator authentication Available is False with OAuthServerDeployment_PreconditionNotFulfilled::OAuthServerServiceEndpointAccessibleController_EndpointUnavailable::OAuthServerServiceEndpointsEndpointAccessibleController_ResourceNotFound::ReadyIngressNodes_NoReadyIngressNodes::WellKnown_NotReady: OAuthServerServiceEndpointAccessibleControllerAvailable: Get \\\"https://10.132.220.152:443/healthz\\\": dial tcp 10.132.220.152:443: connect: connection refused\\nOAuthServerServiceEndpointsEndpointAccessibleControllerAvailable: endpoints \\\"oauth-openshift\\\" not found\\nReadyIngressNodesAvailable: Authentication requires functional ingress which requires at least one schedulable and ready node. Got 0 worker nodes, 2 master nodes, 0 custom target nodes (none are schedulable or ready for ingress pods).\\nWellKnownAvailable: The well-known endpoint is not yet available: failed to get oauth metadata from openshift-config-managed/oauth-openshift ConfigMap: configmap \\\"oauth-openshift\\\" not found (check authentication operator, it is supposed to create this)\"\n</code></pre> <p>Issue: 2 of the master nodes available, requires 3 for etcd cluster.</p> <p>Solution: Check machines, machinesets and nodes : <pre><code>oc --kubeconfig=${INSTALL_DIR}/auth/kubeconfig get clusterversion\noc --kubeconfig=${INSTALL_DIR}/auth/kubeconfig get nodes\noc --kubeconfig=${INSTALL_DIR}/auth/kubeconfig get machines\noc --kubeconfig=${INSTALL_DIR}/auth/kubeconfig get machinesets\noc --kubeconfig=${INSTALL_DIR}/auth/kubeconfig get csr | grep -i pending\noc --kubeconfig=${INSTALL_DIR}/auth/kubeconfig get clusterversion\n\nfor i in `oc get csr --no-headers \\\n| grep -i pending \\ \n| awk '{ print $1 }'`;  do oc adm certificate approve $i; done\n</code></pre></p>","tags":["troubleshooting","openshift"]},{"location":"computerscience/devops/openshift/troubleshooting/#odf-operator-is-not-able-to-upgrade","title":"ODF Operator is not able to upgrade","text":"<p>Issue:  Upgrading ODF operator is not working as expected.</p> <p>Solution: Check OLM components:</p> <ul> <li> <p>Backup install-plan and delete.</p> </li> <li> <p>Backup csv and subscription and delete.</p> </li> <li> <p>Apply subscription again and wait for csv and install-plan to be created.</p> </li> <li> <p>Approve the operator upgrade manually if approve type is manual.</p> </li> <li> <p>Set operator approval type to automatic.</p> </li> </ul>","tags":["troubleshooting","openshift"]},{"location":"computerscience/devops/openshift/troubleshooting/#ocp-upgrade-is-stucked-for-some-of-the-cluster-operators","title":"OCP Upgrade is stucked for some of the cluster operators","text":"<p>Issue: Some of the cluster operators are not able to be upgrade due to the bad conditions.</p> <p>Solution: Check cluster operators and related namespaces: <pre><code>oc get co\n</code></pre> * Check Administration-&gt;Cluster Settings</p> <ul> <li> <p>Check related namespaces to the operators like openshift-sdn, openshift-storage whether or not the pods have underlying issues.</p> </li> <li> <p>Check network connectivity especially if there is cluster-wide proxy.</p> </li> </ul>","tags":["troubleshooting","openshift"]},{"location":"computerscience/devops/openshift/troubleshooting/#bundle-unpacking-failed-reason-deadlineexceeded-while-operator-installation","title":"Bundle unpacking failed. reason deadlineexceeded while operator installation!","text":"<ol> <li> <p>Follow the steps here: https://access.redhat.com/solutions/6459071</p> </li> <li> <p>List the jobs in the namespace openshift-marketplace  <pre><code>oc get job -n openshift-marketplace -o json | jq -r '.items[] | select(.spec.template.spec.containers[].env[].value|contains (\"&lt;operator_name_keyword&gt;\")) | .metadata.name'\n</code></pre></p> </li> <li> <p>Delete the jobs from the ones that were executed above:</p> </li> </ol> <p><pre><code>$ oc delete job &lt;job-name&gt; -n openshift-marketplace\n</code></pre> 4. Delete the configmaps from the ones that were executed above:</p> <pre><code>$ oc delete configmap &lt;job-name&gt; -n openshift-marketplace\n</code></pre> <ol> <li> <p>If the above scripts don't work, continue with steps below:</p> </li> <li> <p>Backup install-plan, csv, subscription</p> </li> <li> <p>Delete install-plan</p> </li> <li> <p>Delete csv</p> </li> <li> <p>Delete subscription</p> </li> <li> <p>Apply same subscription to the cluster.</p> </li> <li> <p>Check that csv and install-plan is created and operator is healthy.</p> </li> <li> <p>Continue upgrading.</p> </li> </ol> <p>IMPORTANT: Deadline in seconds is 600 seconds by default.</p>","tags":["troubleshooting","openshift"]},{"location":"computerscience/devops/podman/podman_debug_level/","title":"Setting debug level for podman","text":"<pre><code>podman --log-level trace login cp.icr.io -u &lt;username&gt; -p &lt;entitlement-key&gt;\n</code></pre>"},{"location":"computerscience/gateway/datapower/administration/administration/","title":"DataPower Administration","text":"","tags":["datapower"]},{"location":"computerscience/gateway/datapower/administration/administration/#creating-an-application-domain-and-api-connect-gateway-service","title":"Creating an application domain and API Connect Gateway Service","text":"<ol> <li> <p>Create a host alias.</p> </li> <li> <p>Create an application domain and ensure it is disabled.</p> </li> <li> <p>Set an ethernet interface for this domain.</p> </li> <li> <p>Create API Connect Gateway Service</p> </li> <li> <p>Select management and api invocation endpoints and related ports for the service.</p> </li> <li> <p>Create key and cert for the domain.</p> </li> <li> <p>Create Identification Credential and bind key and cert to it.</p> </li> <li> <p>Create TLS Client Profile</p> </li> <li> <p>Create TLS Server Profile</p> </li> <li> <p>Create a configuration sequence</p> </li> <li> <p>Check connectivity from apim pod to management and api invocation endpoints.</p> </li> <li> <p>Check connectivity to API Connect from DataPower troubleshooting.</p> </li> </ol>","tags":["datapower"]},{"location":"computerscience/gateway/datapower/architecture/architecture/","title":"API Gateway in DMZ and LAN Reference Architecture","text":"","tags":["datapower"]}]}